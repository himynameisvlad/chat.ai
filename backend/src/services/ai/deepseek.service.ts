import OpenAI from 'openai';
import { IAIProvider } from '../../interfaces/ai-provider.interface';
import { Message, StreamResponse, AppError, DEFAULT_TEMPERATURE } from '../../types';
import { mcpToolsService } from '../mcp/mcp-tools.service';

interface DeepSeekConfig {
  apiKey: string;
  baseURL: string;
  model: string;
  maxTokens: number;
}

export class DeepSeekService implements IAIProvider {
  private client: OpenAI;
  private model: string;
  private maxTokens: number;

  constructor(config: DeepSeekConfig) {
    this.client = new OpenAI({
      apiKey: config.apiKey,
      baseURL: config.baseURL,
    });
    this.model = config.model;
    this.maxTokens = config.maxTokens;
  }

  async streamChat(
    messages: Message[],
    response: StreamResponse,
    customPrompt?: string,
    temperature?: number,
    tools?: OpenAI.Chat.ChatCompletionTool[]
  ): Promise<void> {
    try {
      // Set headers for Server-Sent Events (SSE)
      this.setStreamHeaders(response);

     // const systemPromptContent = customPrompt || this.getDefaultSystemPrompt();

      // const systemMessage: Message = {
      //   role: 'system',
      //   content: systemPromptContent,
      // };

      // const filteredMessages = messages.filter(msg => msg.role !== 'system');
      // const messagesToSend = [systemMessage, ...filteredMessages];

      // Convert messages to OpenAI format
      const formattedMessages: OpenAI.Chat.ChatCompletionMessageParam[] =
        messages.map((msg) => ({
          role: msg.role as 'user' | 'assistant' | 'system',
          content: msg.content,
        }));

      const requestParams: OpenAI.Chat.ChatCompletionCreateParams = {
        model: this.model,
        messages: formattedMessages,
        stream: true,
        temperature: temperature ?? DEFAULT_TEMPERATURE,
        max_tokens: this.maxTokens,
      };

      // Add tools if provided
      if (tools && tools.length > 0) {
        requestParams.tools = tools;
        requestParams.tool_choice = 'auto';
      }

      const stream = await this.client.chat.completions.create(requestParams);

      await this.processStream(stream, response, tools);
    } catch (error) {
      this.handleStreamError(error, response);
    }
  }

  getProviderName(): string {
    return 'DeepSeek';
  }

  /**
   * Sets appropriate headers for Server-Sent Events streaming
   */
  private setStreamHeaders(response: StreamResponse): void {
    response.setHeader('Content-Type', 'text/event-stream');
    response.setHeader('Cache-Control', 'no-cache');
    response.setHeader('Connection', 'keep-alive');
  }

  /**
   * Processes the stream and writes chunks to the response
   */
  private async processStream(
    stream: AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>,
    response: StreamResponse,
    tools?: OpenAI.Chat.ChatCompletionTool[]
  ): Promise<void> {
    const toolCalls: Record<number, { name: string; arguments: string }> = {};

    for await (const chunk of stream) {
      const choice = chunk.choices[0];
      const content = choice?.delta?.content;

      // Handle regular content
      if (content) {
        const data = JSON.stringify({ text: content });
        response.write(`data: ${data}\n\n`);
      }

      // Handle tool calls
      if (choice?.delta?.tool_calls) {
        for (const toolCall of choice.delta.tool_calls) {
          const index = toolCall.index;
          if (!toolCalls[index]) {
            toolCalls[index] = { name: '', arguments: '' };
          }

          if (toolCall.function?.name) {
            toolCalls[index].name = toolCall.function.name;
          }
          if (toolCall.function?.arguments) {
            toolCalls[index].arguments += toolCall.function.arguments;
          }
        }
      }

      // Send token usage when available (typically in final chunk)
      if (chunk.usage) {
        const tokenData = JSON.stringify({
          type: 'token_usage',
          usage: {
            prompt_tokens: chunk.usage.prompt_tokens || 0,
            completion_tokens: chunk.usage.completion_tokens || 0,
            total_tokens: chunk.usage.total_tokens || 0
          }
        });
        response.write(`data: ${tokenData}\n\n`);
      }

      if (choice?.finish_reason === 'tool_calls' && Object.keys(toolCalls).length > 0) {
        // Execute tool calls and send results
        await this.handleToolCalls(toolCalls, response, tools);
        return;
      }

      if (choice?.finish_reason === 'stop') {
        response.write('data: [DONE]\n\n');
      }
    }

    response.end();
  }

  /**
   * Handles tool calls by executing them and streaming the response
   */
  private async handleToolCalls(
    toolCalls: Record<number, { name: string; arguments: string }>,
    response: StreamResponse,
    tools?: OpenAI.Chat.ChatCompletionTool[]
  ): Promise<void> {
    try {
      // Notify user that tools are being executed
      response.write(`data: ${JSON.stringify({ text: '\n\nüîß Using tools...\n\n' })}\n\n`);

      // Execute all tool calls
      const toolResults: Array<{ name: string; result: any }> = [];
      for (const [index, toolCall] of Object.entries(toolCalls)) {
        const { name, arguments: argsStr } = toolCall;

        try {
          const args = JSON.parse(argsStr);
          console.log(`Executing tool: ${name}`, args);

          const result = await mcpToolsService.executeTool(name, args);
          toolResults.push({ name, result });

          response.write(`data: ${JSON.stringify({ text: `‚úì ${name}\n` })}\n\n`);
        } catch (error) {
          console.error(`Error executing tool ${name}:`, error);
          const errorMsg = error instanceof Error ? error.message : 'Unknown error';
          toolResults.push({ name, result: { error: errorMsg } });
          response.write(`data: ${JSON.stringify({ text: `‚úó ${name}: ${errorMsg}\n` })}\n\n`);
        }
      }

      // Format tool results for display
      response.write(`data: ${JSON.stringify({ text: '\n' })}\n\n`);
      for (const { name, result } of toolResults) {
        const resultStr = JSON.stringify(result, null, 2);
        response.write(`data: ${JSON.stringify({ text: resultStr + '\n\n' })}\n\n`);
      }

      response.write('data: [DONE]\n\n');
      response.end();
    } catch (error) {
      console.error('Error handling tool calls:', error);
      this.handleStreamError(error, response);
    }
  }

  /**
   * Handles errors during streaming
   */
  private handleStreamError(error: unknown, response: StreamResponse): void {
    console.error(`[${this.getProviderName()}] Error:`, error);

    if (!response.headersSent) {
      const message = error instanceof Error ? error.message : 'An error occurred';
      throw new AppError(500, `${this.getProviderName()} API error: ${message}`);
    }
  }

  /**
   * Returns the default system prompt for DeepSeek with iterative information gathering
   */
  private getDefaultSystemPrompt(): string {
    return `–¢—ã –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Ä–∞–±–æ—Ç–∞—é—â–∏–π –≤ —Ä–µ–∂–∏–º–µ –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.

–ü–†–û–¶–ï–°–° –†–ê–ë–û–¢–´:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ù–ï–î–û–°–¢–ê–¢–û–ß–ù–û –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ ‚Üí –∑–∞–¥–∞–π 1-2 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–∞
3. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –î–û–°–¢–ê–¢–û–ß–ù–û ‚Üí –¥–∞–π –ø–æ–ª–Ω—ã–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç

–ü–†–ò–ú–ï–†–´:

–ü—Ä–∏–º–µ—Ä 1 - –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:
–ó–∞–ø—Ä–æ—Å: "–ü–æ—Å–æ–≤–µ—Ç—É–π –Ω–æ—É—Ç–±—É–∫"
–û—Ç–≤–µ—Ç: "–ß—Ç–æ–±—ã –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–π –Ω–æ—É—Ç–±—É–∫, —É—Ç–æ—á–Ω–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–º–µ–Ω—Ç–æ–≤:
- –ö–∞–∫–æ–π —É —Ç–µ–±—è –±—é–¥–∂–µ—Ç?
- –î–ª—è –∫–∞–∫–∏—Ö –∑–∞–¥–∞—á –±—É–¥–µ—à—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (—Ä–∞–±–æ—Ç–∞, –∏–≥—Ä—ã, —É—á–µ–±–∞)?"

–ü—Ä–∏–º–µ—Ä 2 - –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:
–ü–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤: "–ù–∞ –æ—Å–Ω–æ–≤–µ —Ç–≤–æ–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π (–±—é–¥–∂–µ—Ç 50–∫, –¥–ª—è —Ä–∞–±–æ—Ç—ã –∏ —É—á–µ–±—ã) —Ä–µ–∫–æ–º–µ–Ω–¥—É—é MacBook Air M2. –û–Ω –æ—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è..."

–ü—Ä–∏–º–µ—Ä 3 - –ü—Ä–æ—Å—Ç–æ–π –≤–æ–ø—Ä–æ—Å:
–ó–∞–ø—Ä–æ—Å: "–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 2+2?"
–û—Ç–≤–µ—Ç: "2 + 2 = 4"

–ü–†–ê–í–ò–õ–ê:
- –ù–ï –¥–∞–≤–∞–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, –ø–æ–∫–∞ –Ω–µ –ø–æ–ª—É—á–∏—à—å –í–°–Æ –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
- –ó–∞–¥–∞–≤–∞–π 1-2 –≤–æ–ø—Ä–æ—Å–∞ –∑–∞ —Ä–∞–∑ (–Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
- –í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∏ –≤–∞–∂–Ω—ã–º–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
- –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—Ä–æ—Å—Ç–æ–π –∏ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–π - —Å—Ä–∞–∑—É –¥–∞–≤–∞–π –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
- –ù–ï –∑–∞–¥–∞–≤–∞–π –æ—á–µ–≤–∏–¥–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω—É–∂–Ω—ã
- –î–µ—Ä–∂–∏ –≤ –ø–∞–º—è—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –∏ –æ—Ç–≤–µ—Ç–∞—Ö
- –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º, –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è`;
  }
}
